---
title: "Data for slides"
author: "Dr. Stephen W. Thomas, Queen's University"
date: "2017"
output:
  pdf_document:
    highlight: pygments
    number_sections: yes
    toc: no
    toc_depth: '2'
---



```{r}
library(tidytext)
library(tidyr)
library(dplyr)
library(ggplot2)
library(readr)
library(tm)
library(jsonlite)
library(rjson)
library(data.table)
```

# Amazon Reviews


introduce dataset:
- titles, dates, etc.

Throughout: show examples of text trickery.

words
words over time
log ratios
n-grams
skip grams

word clusters

doc clusters

doc classification

topic modeling

## Read in the data

```{r}

# The file is, for some reason, in a strange format, where each line is its own JSON.
dat_full <- lapply(readLines('reviews_Grocery_and_Gourmet_Food_5_50000.json'), fromJSON)

# The following calls fail with parse errors, due to the above
#dat = read_json('reviews_Grocery_and_Gourmet_Food_5.json', simplifyVector = TRUE)
#dat = fromJSON(file='reviews_Grocery_and_Gourmet_Food_5.json')
```


## Take a sample

```{r}

# Take a sample of the data, just for ease of use
sample = FALSE
if (sample == TRUE) {
  n = 5000
  set.seed(1111)
  dat = sample(dat_full, n)
} else {
  n = length(dat_full)
  dat = dat_full
}


# Create a dataframe out of the data.
# (Actually, use a data.table for speed. Also use the set() method for great speedup.)
# The following code is an ugly hack. Unfortunately, simple methods like as.data.frame were not working,
# and in the interest of time, I just brute forced the creation of a dataframe by looping through
# the list of json docs manually.
df_a = data.table(
  reviewID = rep(1L, n),
  reviewerID = rep("", n),
  asin = rep("", n),
  reviewerName = rep("", n),
  reviewText = rep("", n),
  overall = rep(-1L, n) ,
  summary = rep("", n),
  unixReviewTime = rep(-1L, n),
  stringsAsFactors = FALSE
  )

i = 1L
for (u in dat) {
  # Some reviews have a NULL/empty reviewer name.
  if (!exists("reviewerName", where = u)) {
    rn = ""
  } else {
    rn = u$reviewerName
  }
  set(df_a, i=i, j=1L, value=i)
  set(df_a, i=i, j=2L, value=u$reviewerID)
  set(df_a, i=i, j=3L, value=u$asin)
  set(df_a, i=i, j=4L, value=rn)
  set(df_a, i=i, j=5L, value=u$reviewText)
  set(df_a, i=i, j=6L, value=u$overall)
  set(df_a, i=i, j=7L, value=u$summary)
  set(df_a, i=i, j=8L, value=u$unixReviewTime)
  i = i+1L
}

dim(df_a)
head(df_a)
str(df_a)

df_a[4,]$reviewText

df_a %>%
  filter(reviewID==24327)
```




## Create tidy format

```{r}
text_df_a <- df_a %>% 
  unnest_tokens(word, reviewText)

# Number of words
dim(text_df_a)

# Number of unique words
text_df_a %>%
  select(word) %>%
  distinct(word) %>%
  summarize(total = n())
```


## Uni-gram frequency analysis

```{r}

iwidth = 7
iheight = 7

theme_set(theme_gray(base_size = 18))

text_df_a %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  count(word, sort=TRUE) %>%
  top_n(100)

text_df_a %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  count(word, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()


ggsave(file="amazon_food_1.pdf", width=iwidth, height=iheight)
```

## Uni-gram frequency analysis, over time

```{r}
#install.packages("directlabels")
library(directlabels)

iwidth = 9
iheight = 5

theme_set(theme_gray(base_size = 18))

text_df_a %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  filter(word %in% c("coffee", "tea", "chocolate", "milk", "cheese", "pasta")) %>%
  mutate(dt = as.POSIXct(unixReviewTime, origin="1970-01-01")) %>%
  mutate(month = format(dt, "%m"), year = format(dt, "%Y")) %>%
  filter(year > 2006) %>%
  filter(year < 2014) %>%
  count(word, year, sort=TRUE) %>%
  ggplot(aes(year, n, group=word, color=word)) +
  geom_line(aes(color=word)) +
  labs(x = "year", y = "n") + 
  scale_colour_discrete(guide = 'none')  +    
  expand_limits(x=8) +
  geom_dl(aes(label = word), method = list(dl.trans(x = x + .2), "last.points")) 

ggsave(file="amazon_food_time.pdf", width=iwidth, height=iheight)
```


## Word frequency density plot

```{r}

tmp = text_df_a %>%
  count(word, sort=TRUE)

head(tmp)
dim(tmp)


iwidth = 9
iheight = 5

theme_set(theme_gray(base_size = 18))

tmp %>%
  filter(n > 1) %>%
  ggplot(aes(reorder(word, -n), n)) +
  geom_col() + 
  theme(axis.text.x = element_blank(), axis.ticks = element_blank()) + 
  labs (x = "rank", y = "count") +
  scale_y_log10()

ggsave(file="amazon_food_hist.pdf", width=iwidth, height=iheight)
```


## Word frequency for a certain target (i.e., rating).
```{r}

iwidth = 7
iheight = 7

theme_set(theme_gray(base_size = 18))

text_df_a %>%
  filter(overall >= 3) %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  count(word, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()


ggsave(file="amazon_good.pdf", width=iwidth, height=iheight)

text_df_a %>%
  filter(overall < 3) %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  count(word, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()


ggsave(file="amazon_bad.pdf", width=iwidth, height=iheight)
```


## Log ratios

```{r}

tmp = text_df_a %>%
  mutate(overall_str = ifelse(overall >= 3, "positive", "negative"))


status_words_count = tmp %>% group_by(overall_str, word) %>%
  summarize(count=n()) %>%
  arrange(desc(count))

head(status_words_count)

log_ratios = status_words_count %>% 
  spread (overall_str, count) %>%
  mutate(negative = ifelse(is.na(negative), 0, negative)) %>%
  mutate(positive = ifelse(is.na(positive), 0, positive)) %>%
  mutate(total=negative+positive) %>%
  mutate(log_ratio = log2((positive+1)/(negative+1))) 

log_ratios

log_ratios %>%
  filter(total > 50) %>%
  group_by(log_ratio < 0) %>%
  top_n(15, abs(log_ratio))


iwidth = 9
iheight = 5

theme_set(theme_gray(base_size = 18))

log_ratios %>%
  filter(total > 100) %>%
  group_by(log_ratio < 0) %>%
  top_n(20, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio)) %>%
  ggplot(aes(word, log_ratio, fill = log_ratio > 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio") +
  scale_fill_discrete(name = "", labels = c("negative", "positive"))


ggsave(file="amazon_food_logodds.pdf", width=iwidth, height=iheight)
```

## N-Gram analysis

```{r}

text_bigrams_a <- df_a %>%
  unnest_tokens(bigram, reviewText, token = "ngrams", n = 3)


iwidth = 7
iheight = 7

theme_set(theme_gray(base_size = 18))

text_bigrams_a %>%
  count(bigram, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(bigram, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()

ggsave(file="amazon_food_3gram.pdf", width=iwidth, height=iheight)

iwidth = 7
iheight = 7

theme_set(theme_gray(base_size = 18))

text_bigrams_a %>%
  count(bigram, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(bigram, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()

ggsave(file="amazon_food_3gram.pdf", width=iwidth, height=iheight)

text_bigrams_a %>%
  filter(grepl("good$", bigram)) %>%
  count(bigram, sort=TRUE)

text_bigrams_a %>%
  filter(grepl("bad$", bigram)) %>%
  count(bigram, sort=TRUE)

text_bigrams_a %>%
  count(bigram, sort=TRUE)
```

## Skip-gram analysis

```{r}
install.packages("quanteda")
library(quanteda)

# TODO:
# Upgrade to OSX 10.11
# Upgrade to R 3.4.3

```

# Reuters


## Read in the data

```{r}
df_r = read_csv("reutersCSV.csv")

head(df_r)
str(df_r)

# Not the huge list of dummy variables. Don't really want to collapse them down, because 
# each row may be marked by 0, 1, or more topics.

df_r[3,]$doc.text
```

## Conver to tidy format

```{r}

text_df_r <- df_r %>% 
  unnest_tokens(word, doc.text)

# Number of words
dim(text_df_r)

# Number of unique words
text_df_r %>%
  select(word) %>%
  distinct(word) %>%
  summarize(total = n())
```


## Uni-gram frequency analysis


All words.
```{r}

iwidth = 7
iheight = 7

theme_set(theme_gray(base_size = 18))

text_df_r %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  count(word, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()


ggsave(file="reuters_1.pdf", width=iwidth, height=iheight)
```

## Word frequency density plot

```{r}
tmp = text_df_r %>%
  count(word, sort=TRUE)

head(tmp)
dim(tmp)

iwidth = 9
iheight = 5

theme_set(theme_gray(base_size = 18))

tmp %>%
  filter(n > 1) %>%
  ggplot(aes(reorder(word, -n), n)) +
  geom_col() + 
  theme(axis.text.x = element_blank(), axis.ticks = element_blank()) + 
  labs (x = "rank", y = "count") +
  scale_y_log10()

ggsave(file="reuters_hist.pdf", width=iwidth, height=iheight)
```


## Word frequency for a certain target (i.e., retuers topic).
```{r}

iwidth = 7
iheight = 7

theme_set(theme_gray(base_size = 18))

text_df_r %>%
  filter(topic.coffee == 1) %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  count(word, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()


ggsave(file="reuters_coffee.pdf", width=iwidth, height=iheight)


text_df_r %>%
  filter(topic.housing == 1) %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  count(word, sort=TRUE) %>%
  top_n(20) %>%
  ggplot(aes(reorder(word, n), n)) +
  geom_col() +
  labs(x = NULL, y = "n") +
  coord_flip()


ggsave(file="reuters_housing.pdf", width=iwidth, height=iheight)
```

## N-Gram analysis

```{r}

text_bigrams_r <- df_r %>%
  unnest_tokens(bigram, doc.text, token = "ngrams", n = 4)

text_bigrams_r %>%
  filter(grepl("^united states", bigram)) %>%
  count(bigram, sort=TRUE)

text_bigrams_r %>%
  filter(grepl("good$", bigram)) %>%
  count(bigram, sort=TRUE)

text_bigrams_r %>%
  filter(grepl("bad$", bigram)) %>%
  count(bigram, sort=TRUE)

text_bigrams_r %>%
  count(bigram, sort=TRUE)
```