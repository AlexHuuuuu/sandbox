---
title: "Data and code for preprocessing slides"
author: "Dr. Stephen W. Thomas, Queen's University"
output:
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
  pdf_document:
    highlight: pygments
    number_sections: yes
    toc: no
    toc_depth: '2'
---



```{r}
library(tidyr)
library(dplyr)
library(readr)
library(tm)
library(qdap)
library(ngram)
```



# Case Normalization

Case folding:

```{r}
base::tolower("Hi, my name is Steve.")
base::tolower("I work at the U.N. in NYC in the USA.")
```

No support for truecasing (that I know of).


# Tokenization

```{r}
#library(devtools)
#install_github("ropensci/tokenizers")
```

```{r}
library(tokenizers)
tokenizers::tokenize_words("Hi, my name is Steve.", lowercase=F)

tokenizers::tokenize_words("Let's go to the U.N. in NYC!!", lowercase=F)

tokenizers::tokenize_words("The ever-popular San Fransisco love letter arrived on March 11, 2019.", lowercase=F)

tokenizers::tokenize_words("stephen.thomas@queensu.ca", lowercase=F)

tokenizers::tokenize_words("(613) 453-6162", lowercase=F)
```



# N-Grams

```{r}
ng = ngram::ngram("Hey there, I'm awesome.", n=2)
print(ng, output="full")

```



# Removing Characters

```{r}
# Removing punctuation
tm::removePunctuation("Hey! Let's go to the bar...")

# Removing special characters
astr <- "Ábcdêãçoàúü"
iconv(astr, to = "ASCII//TRANSLIT")
```


# Removing Numbers

```{r}
tm::removeNumbers("There are only 4 classes left.")

```

# Regular Expressions

```{r}
story=c(
  "Alejandrina has a small store in her house where she sells basic products [such as soap, cooking oil, eggs, etc.] and stationary products. Alejandrina started her business with the help of her husband and with $. of capital. Translated from Spanish by Jennifer Day, Kiva Volunteer.",
  "Nancy works as a saleswoman, her main product being used clothing. She buys packs of clothes and resells them to friends and neighbors. She began her business with her family's help, who live abroad and sent her her first shipment of clothes. At the beginning, she only sold used clothing items. Now she sells new clothing as well as cosmetics.Translated from Spanish by Kiva Volunteer, Kristin Connor.",
  "Marjorie sells lunch to companies and construction workers. He started her business  years ago with the help of her sister who lent her money for starting up. Her sister also helped her to find customers. \r\n\r\nIn the beginning, she only sold lunch. Translated by Ramn F. Kolb.")

gsub("Translated[^\\.]+\\.", " ", story, ignore.case=TRUE)
```



# Stemming

```{r}
qdap::stemmer("We are writing code like hackers.", capitalize = FALSE)
```

# Lemmatization

Can use textstem::lemmatize_words or koRpus::treetag

```{r}
library(koRpus)
tagged.results <- treetag(c("run", "ran", "running"), treetagger="manual", format="obj",
                      TT.tknz=FALSE , lang="en",
                      TT.options=list(path="./TreeTagger", preset="en"))
tagged.results@TT.res

```

# Spell Checking

Hunspell is the spell checker library used by LibreOffice, OpenOffice, Mozilla Firefox, Google Chrome, Mac OS-X, InDesign, Opera, RStudio and many others.

```{r}
library(hunspell)

hunspell_check(c("beer", "wiskey", "wine"))

hunspell_suggest("wiskey")
```

```{r}
qdap::check_spelling("This is not spelld correcly.")
```


# Stopping

```{r}
qdap::rm_stop("My name is Steve, and I am a good chef.", stopwords = qdapDictionaries::Top200Words)
```


# Removing Rare Words

```{r}
myText <- c("the quick brown furry fox jumped over a second furry brown fox",
              "the sparse brown furry matrix",
              "the quick matrix")

require(tm)
myVCorpus <- VCorpus(VectorSource(myText))
myTdm <- DocumentTermMatrix(myVCorpus)
as.matrix(myTdm)
as.matrix(removeSparseTerms(myTdm, .01))
as.matrix(removeSparseTerms(myTdm, .99))
as.matrix(removeSparseTerms(myTdm, .5))

   
require(quanteda)
myDfm <- dfm(myText, verbose = FALSE)
docfreq(myDfm)
dfm_trim(myDfm, min_count = 2)
```

# All in One

With textmineR:

```{r}
df = data.frame(ID = 1:3, 
                Text=c("My dog ate my homework.", 
                        "The cat ate my sandwich.", 
                        "A dolphin ate the homework and the sandwich."))
library(textmineR)
dtm <- textmineR::CreateDtm(doc_vec = df$Text,
                 doc_names = df$ID, 
                 ngram_window = c(1, 2), 
                 stopword_vec = c(tm::stopwords("english"), 
                                  tm::stopwords("french"), 
                                  tm::stopwords("spanish")), 
                 lower = TRUE, 
                 remove_punctuation = TRUE, 
                 remove_numbers = TRUE,
                 verbose = FALSE,
                 stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"))

as.data.frame(as.matrix(dtm), stringsAsFactors=False)
```

# R Data Types and Formats

```{r}
df = data.frame(ID = 1:3, 
                Text=c("My dog ate my homework.", 
                        "The cat ate my sandwich.", 
                        "A dolphin ate the homework and the sandwich."), stringsAsFactors = FALSE)
```



```{r}
require(tm)
myVCorpus <- VCorpus(VectorSource(myText))
dtm <- DocumentTermMatrix(myVCorpus)
str(dtm)
dim(dtm)
```


```{r}
require(textmineR)
dtm <- textmineR::CreateDtm(doc_vec = df$Text, doc_names = df$ID)
str(dtm)
dim(dtm)
```


```{r}
require(text2vec) 
prep_fun = tolower
tok_fun = word_tokenizer

it_train = itoken(df$Text, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = df$ID, 
             progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab
str(vocab)
dim(vocab)

vectorizer = vocab_vectorizer(vocab)
dtm = create_dtm(it_train, vectorizer)
str(dtm)
dim(dtm)
as.data.frame(as.matrix(dtm))


h_vectorizer = hash_vectorizer(hash_size = 2 ^ 5, ngram = c(1L, 2L))
dtm_h = create_dtm(it_train, h_vectorizer)
str(dtm_h)
dim(dtm_h)
as.data.frame(as.matrix(dtm_h))

```


```{r}
require(tidytext)
tidy <- df %>% 
  unnest_tokens(word, Text)

str(tidy)
dim(tidy)
```

# Fuzzy Matching

Good research article: https://journal.r-project.org/archive/2014-1/loo.pdf

Also see fuzzywuzzyR.

```{r}
good_names <- data.frame(
  name = c('Queen\'s University', 'University of Toronto', 'McGill University', 'University Of Waterloo'),
  location = c("Kingston, ON", "Toronto, ON", "Montreal, QB", "Waterloo, ON"))

b <- data.frame(name = c('Queens University', 'Queen\'s', 'Queens Univrsity',
                         'U of T', 'Toronto', 'Rotman',
                         'Mcgill', 'McGill U.',
                         'Waterloo', 'Waterloop University'))

library(fuzzyjoin)
library(dplyr)

res = stringdist_join(b, good_names, 
                by = "name",
                mode = "left",
                ignore_case = TRUE, 
                method = "jw", 
                max_dist = 99, 
                distance_col = "dist") %>%
  group_by(name.x) %>%
  top_n(1, -dist) 

res

res %>%
  write.csv("out/fuzzy_matches.csv")
```

# Vectorization

Use text2vec to compare BOW to feature hashing embeddings on Kiva Dataset


```{r}
require(textclean)
require(tidyverse)
require(rpart)
require(rpart.plot)
require(text2vec) 

df <- read_csv("data/kiva.csv")
df = df %>%
  rename(story = en) %>%
  mutate(story = gsub("<.*?>", "", story)) %>% # Remove HTML tags
  mutate(story = gsub("\\d", "", story)) # Remove digits


df$id = 1:nrow(df)
df$status = as.factor(df$status)
df$sector = as.factor(df$sector)
df$country = as.factor(df$country)
df$gender = as.factor(df$gender)
df$nonpayment = as.factor(df$nonpayment)

prep_fun = tolower
tok_fun = word_tokenizer

it_train = itoken(df$story, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = df$id, 
             progressbar = FALSE)
vocab = create_vocabulary(it_train, ngram = c(1L, 3L))
pruned_vocab = prune_vocabulary(vocab, 
                                 term_count_min = 10, 
                                 doc_proportion_max = 0.5,
                                 doc_proportion_min = 0.01)

# define tfidf model
tfidf = TfIdf$new()

vectorizer = vocab_vectorizer(pruned_vocab)
dtm = create_dtm(it_train, vectorizer)
dtm = fit_transform(dtm, tfidf)
dim(dtm)

# rf is picky about column names like "if" or "while"
colnames(dtm) <- paste0(("bow_"), colnames(dtm))

df2 = cbind(df, as.matrix(dtm))

set.seed(123)
# Don't want to use either of these for prediction, and the - sign doesn't work
# with rpart forumulas.
colnames(df)
df_text = subset(df2, select=c(-id, -story, -nonpayment, -country, -sector, -gender, -loan_amount))
dim(df_text)



# Split the data into training and testing.
train_text <- sample_frac(df_text, 0.8)
test_text <- setdiff(df_text, train_text)


# Let's create the model.
form = as.formula(status ~ .)
tree.text <- rpart(form, train_text, method="class")
tree.text
rpart.plot(tree.text, extra=2)
pred = predict(tree.text, train_text, type="class")
head(pred)
caret::confusionMatrix(pred, train_text$status)




require(randomForest)
set.seed(111)
rf = randomForest(form, data=train_text, importance=TRUE, nodesize=42, mtry=10, ntree=100)
round(importance(rf), 2)
varImpPlot(rf, n.var=10)
```

FH:

```{r}
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))
dtm_h = create_dtm(it_train, h_vectorizer)
str(dtm_h)
dim(dtm_h)
as.data.frame(as.matrix(dtm_h))
```

