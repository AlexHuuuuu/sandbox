---
title: "Naive Bayes - Employee Attrition"
output: 
  html_document:
      toc: yes
      toc_float: yes
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction

Given the potential disruption to the work environment and the required resources to attract, acquire, and train new talent, understanding factors that influence employee attrition is important to human resource departments. In this exercise, we'll explore the IBM Human Resources Analytics dataset, which contains data on employee attrition (whether an employee will leave the company).  Throughout this exercise, we'll review basic data wrangling and visualization, as well as classification algorithms such as Naive Bayes, k-Nearest Neighbors, and Support Vector Machines. Additionally, we will compare the models using classification metrics and explain why certain metrics can be misleading.

You'll notice on the right hand side of this document a number of dropdown menus that can be used to unhide the code used to generate the outputs. There will be questions guiding you through this analysis that ask you certain questions about the data. Please use the accompanying practice RMarkdown file to try and generate the correct outputs on your own before peeking! If you get stuck, we have added links throughout the document to point you in the right direction.

##Packages

First, let's have a look at some of the packages that will be used in this exercise:

- ```rsample``` contains the IBM attrition data set called ```attrition```, also helpful for sampling data
- ```dplyr``` is a popular package for manipulating data in R
- ```ggplot2``` is robust and flexible data visualization package
- ```caret``` is a statisitcal / machine learning wrapper or framework for other ML packages
- ```corrplot``` is a visualization library specifically for correlation

If you don't have these installed, please install them now. If you're not sure how to do this, try the ```install.packages()``` function with the package name in quotes. 

```{r install_packages, warning = FALSE, message = FALSE}

# install.packages("knitr")
# install.packages("tibble")
# install.packages("rsample")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("corrplot")
# install.packages("corrr")
# install.packages("caret")

```

Once installed, we need to load the packages into our library using the ```library()``` function. Be sure to watch out for errors when loading libraries! Sometimes two libraries will share a common function name, and loading them both will cause conflicts known as "masking".

```{r load_packages, warning = FALSE, message = FALSE}
library(tibble)
library(dplyr)    # data transformation
library(rsample)  # data splitting 
library(ggplot2)  # data visualization
library(corrplot)
library(caret) # implementing with caret 
library(corrr)
library(knitr)
```

##Data Exploration

The data that we will be using was already loaded into memory when we loaded the ```rsample``` library. Reading data into R is beyond the scope of this document, but interested students should have a look at [this online tutorial](https://www.datacamp.com/community/tutorials/r-data-import-tutorial).

Now that the data is available to us, let's start by exploring the data set a bit. There are a number of ways to do this, but using the ```dim()```, ```names()```, and ```str()``` functions are a good place to start. If you're not sure what they mean, try typing ```?dim``` in the console to read the documentation. 

What is the dimension of the dataset?

```{r, warning = FALSE, message = FALSE}
attrition %>% dim()
```

**Note:** For those following along in RStudio, you may be wondering what the ```%>%``` is doing here. This is called the "pipe operator", which passes whatever is on the left into whatever is on the right. While it's not immediately obvious why this might be useful, the [following tutorial](https://www.datacamp.com/community/tutorials/pipe-r-tutorial) provides some helpful information about its historical use in programming languages and how it works in R. At their core, pipe operators make your code more readable by avoiding nested functions. 

So we have a dataset with 31 columns and 1470 rows, but what exactly does the dataset look like? What are the names of the columns in our data? *Hint:* Try the ```names()``` function. 

```{r, warning = FALSE, message = FALSE}
attrition %>% names() %>% kable()
```

Now we know which features are available to us, but what *kind* of data is it? We could use the ```class()``` function to return the data type on an individual column, but the ```str()``` function can be used to tell us the data type of all columns in the dataset, including the different levels of categorical data, a small sample of each data, and many other helpful pieces of information. Similarly, try the ```glimpse()``` function for another view.

**Question:** Based on the above below, are there any data types that you think need to be changed? Take ```JobLevel``` for instance, should that really be an integer? Or does it feel more like a category variable? The code chunk below changes the datatype of some columns - but feel free to try some others!

```{r, warning = FALSE, message = FALSE}
attrition %>% str()
```

```{r, warning = FALSE, message = FALSE}
attrition <- attrition %>%
  mutate(
    JobLevel = factor(JobLevel),
    StockOptionLevel = factor(StockOptionLevel),
    TrainingTimesLastYear = factor(TrainingTimesLastYear),
    Attrition = factor(Attrition, levels = c("Yes", "No"))
  )
```

**Note:** If this is your first time using the ```dplyr``` library, you might be wondering what ```mutate``` does. You can think of this function as just another way to create a variable in a dataframe. The ```dplyr``` library is one of the best ways in R to wrangle / transform your data so that it's in a format that is easily digestible by models or plotting code. The [following course](https://www.datacamp.com/courses/dplyr-data-manipulation-r-tutorial) is a great introduction to the many different transformations you can do. 


Let's see if we can spot some basic patterns. Try using the ```table()``` function to calculate a count basic table from two categorical variables. You then pass the count table into the ```prop.table()``` function to return the percentages. 

**Question:** Have a look below. How does ```JobSatisfaction``` impact ```Attrition```?

```{r}
df <- attrition
table(df$JobSatisfaction, df$Attrition) %>% 
  prop.table() %>% 
  kable()
```

**Question:** What about ```WorkLifeBalance```? 

```{r}
table(df$WorkLifeBalance, df$Attrition) %>% 
  prop.table() %>% 
  kable()
```

##Data Visualization

Often times, you're going to be working with datasets that have many, many variables, and looking for patterns one by one is not a feasible solution. Let's use some visual techniques to identify patterns more efficiently. Do any interesting patterns emerge?

**Note:** Visualizing categorical and numerical data can be accomplished in many different ways, but using density plots for numeric data and bar plots for categorical data is a good place to start start. The ```ggplot2``` library uses a philosophy known as the "grammar of graphics", and while very powerful once understood, does come with its own learning curve. Thankfully, there is a [three-part series](https://www.datacamp.com/courses/data-visualization-with-ggplot2-1) on the topic to get you started. Until then, here is one visualization approach that hopefully gets you excited to learn the library.

**Numerical Variables:**
```{r, warning = FALSE, message = FALSE, fig.height = 8}
numeric_attrition <- attrition %>% 
  select(which(sapply(., class)=="integer"), Attrition) 

numeric_attrition %>% 
  gather(metric, value, -Attrition) %>% 
  ggplot(aes(value, fill = Attrition)) + 
  geom_density(show.legend = TRUE, alpha = 0.75) + 
  facet_wrap(~ metric, scales = "free", ncol = 3) +
  theme_bw() +
  labs(x = "", y = "")
```

**Categorical Variables:**
```{r,warning = FALSE, message = FALSE, fig.height = 8}
categoric_attrition <- attrition %>% 
  select(which(sapply(., class)=="factor"), Attrition) %>% 
  select(which(sapply(., nlevels)<=5))

categoric_attrition %>% 
  gather(metric, value, -Attrition) %>% 
  ggplot(aes(value, fill = Attrition)) + 
  geom_bar(position = "dodge", col = "black") + 
  facet_wrap(~ metric, ncol = 3, scales = "free") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = "", y = "")
  
```

Another way to look at the data is to use a correlation plot, which shows how each variable is correlated to every other variable. For simplicity, we'll only be looking at the correlation between numeric data, but [this resource](https://www.r-bloggers.com/to-eat-or-not-to-eat-thats-the-question-measuring-the-association-between-categorical-variables/) provides an approach for categorical data. 

For the numerical correlation plot, we need to first calculate the correlation matrix using the ```cor()``` function. Then, we pass the resulting matrix into ```corrplot()```, which outputs our correlation plot. 

**Question:** Does anything from the correlation plot stand out? Try combining your observations from the previous visualizations and see if you can come up with some hypotheses about why employees leave. Remember these for later on when we get to the results!

```{r, message = FALSE, warning = FALSE}
numeric_attrition %>% 
  select(-Attrition) %>% 
  cor() %>% 
  corrplot(method = "shade", type = "lower")
```

##Modeling

###Naive Bayes

Before we use the Naive Bayes algorithm on the entire dataset, let's try practicing it on a small subset of the data. Let's pretend we only have the first ten rows of two explanatory variables of interest: ```Gender``` and ```BusinesTravel```. 

**Question:** Given this dataset, what is the probability that a **male who travels rarely** will leave the company? To answer this, you'll need the following information. If you get stuck, try following [this tutorial](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) to point you in the right direction. Good luck!

**Naive Bayes Tables**
```{r}
subset <- attrition %>% select(Attrition, Gender, BusinessTravel) %>% head(10)

kable(subset, align = "c")

table(subset$Attrition)# %>% kable(align = "c", col.names = c("Attrition", "Frequency"))
table(subset$Attrition, subset$Gender)# %>% kable(align = "c")
table(subset$Attrition, subset$BusinessTravel) #%>% kable(align = "c")

```

###Implementation

The ```caret``` package is one of the most popular machine learning libraries in R because it provides a common way (framework) to train machine learning models. It acts as a wrapper around packages with different algorithm implementations, so instead of having to learn the syntax for every single package, you can just use common functions in ```caret```. The author of the package, Max Kuhn, maintains an [active documentation / tutorial](https://topepo.github.io/caret/) website that contains everything you need to know about the library. He also happens to be the author of your textbook, "Applied Predictive Modeling", which *also* has an entire package dedicated to it, aptly named ```AppliedPredictiveModeling```.

Before we start building our model, we need to split the data into training and testing sets. Here we'll use the ```initial_split()``` function, which essentially creates an index from which the ```training()``` and ```testing()``` functions extract the data.

**Question:** Have a look at the code chunk below - what do you think the ```strata = ``` argument is doing? *Hint:* Take a look at the proportions of the ```Attrition``` categories in both the training / testing set below. 

```{r, message = FALSE, warning = FALSE}
set.seed(1234)

split <- initial_split(attrition, prop = .7, strata = "Attrition")
train <- training(split)
test  <- testing(split)
```

```{r, message = FALSE, warning = FALSE}
table(train$Attrition) %>% prop.table() %>% kable(col.names = c("Training Set: Attrition", "Freq (%)"), align = "c")
```

```{r, message = FALSE, warning = FALSE}
table(test$Attrition) %>% prop.table() %>% kable(col.names = c("Testing Set: Attrition", "Freq (%)"), align = "c")
```

###Results

In ```caret```, the ```train()``` function is the primary way to build machine learning models, and the argument ```method``` is used to specify which implementation you want to use. Under the hood, the ```method``` argument retrieves the desired package implementation, which is why you may be prompted to install a package if you're trying a new algorithm. [Here's](https://topepo.github.io/caret/available-models.html) a list of all 237 possible algorithm implementations - today, we'll be using the ```naive_bayes``` method. Once built, we will use the ```predict()``` function on the test set to get our estimates, and then use the ```confusionMatrix()``` function to evaluate performance. For the sake of comparison, we've also trained a k-Nearest Neighbours (kNN) and a Support Vector Machine (SVM) model, which you will learn about a little later on in the lecture. 

**Question:** The results of the NB, kNN, and SVM models are presented below. Calculate the accuracy of each model using the provided confusion matrix. Which model would you select?

**Naive Bayes Test Confusion Matrix:**
```{r, message = FALSE, warning = FALSE}
nb.m1 <- train(
  Attrition ~ .,
  data = train,
  method = "naive_bayes"
  )

predictions_nb <- predict(nb.m1, test)
actuals <- test$Attrition

# Testing Results
conf_matrix_nb <- confusionMatrix(predictions_nb, actuals)
conf_matrix_nb$table
```

**k-Nearest Neighbours Test Confusion Matrix:**
```{r, message = FALSE, warning = FALSE}
knn.m1 <- train(
  Attrition ~ .,
  data = train,
  method = "knn"
  )

predictions_knn <- predict(knn.m1, test)

# Testing Results
conf_matrix_knn <- confusionMatrix(predictions_knn, actuals)
conf_matrix_knn$table
```

**Support Vector Machine Test Confusion Matrix:**
```{r, message = FALSE, warning = FALSE}
svm.m1 <- train(
  Attrition ~.,
  data = train,
  method = "svmLinear"
  )

# Testing Results
predictions_svm <- predict(svm.m1, test)

# Testing Results
conf_matrix_svm <- confusionMatrix(predictions_svm, actuals)
conf_matrix_svm$table
```

##Discussion

Given the results below, you may be inclined to choose the SVM model. But is accuracy the only metric that matters? Go back and have another look at how well each model correctly classifies **just the employees that leave**. How would your model selection change? Why? Is accuracy the only metric that matters in classification problems?

```{r, message = FALSE, warning = FALSE}
accuracy_nb <- round(conf_matrix_nb$overall[1], 4)*100
accuracy_knn <- round(conf_matrix_knn$overall[1], 4)*100
accuracy_svm <- round(conf_matrix_svm$overall[1], 4)*100

accuracy <- data.frame(Model = c("NB", "kNN", "SVM"), Accuracy = c(accuracy_nb, accuracy_knn, accuracy_svm))

ggplot(accuracy, aes(x = Model, y = Accuracy)) + geom_col(fill = "blue", alpha = 0.6, col = "black") + labs(x = "", y = "Accuracy (%)") + ylim(0, 100) + theme_bw() + labs(title = "Comparison of NB, kNN, and SVM Results")
```

**Question:** The table below shows the relative importance of each feature in the Naive Bayes model as calculated by ```caret```. Do the importance of these features surprise you? Can you come up with a hypotheses as to why employees leave?

```{r}

varImp(nb.m1)$importance %>% add_rownames("Feature") %>% select(Feature, Importance = Yes) %>% mutate(Importance = round(Importance, 2)) %>% arrange(desc(Importance)) %>% kable(col.names = c("Feature", "Relative Importance (%)"), align = "c")

```

