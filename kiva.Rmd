---
title: "Classification Case Study: Kiva Loans"
output: html_document
author: "Stephen W. Thomas"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
library(tidyverse)
library(scales)
library(titanic)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(MLmetrics)
library(topicmodels)
library(tidytext)
```


# Introduction

TODO


## Learning Objectives

- Understand and interpret the results of a decision tree classifier model.
- Use a decision tree classifier model to predict new data instances.
- Determine if text helps classification results.
- Learn who pays back loans, and who doesn't.

# Loading the Data

TODO


```{r}
df <- read_csv("kiva.csv")
```



A good idea when performing any data analytics activity is to spend time looking at some of the raw data. 

```{r}
str(df)
df$id = 1:nrow(df)
df$status = as.factor(df$status)
df$sector = as.factor(df$sector)
df$country = as.factor(df$country)
df$gender = as.factor(df$gender)
df$nonpayment = as.factor(df$nonpayment)
str(df)
head(df, n=20)
summary(df)
```

# Data Cleaning

Clean up some of that English!

```{r}

```


# Feature Engineering


Tidy up the text.

```{r}
text_df <- df %>%
  select(id, status, en) %>%
  unnest_tokens(word, en)
```

Look at ngrams.

```{r}
  #unnest_tokens(ngram, en, token="ngrams", n=1)
```

# Remove stopwords

```{r}
custom_stop_words = data.frame(word=c("br", "h4", "loan", "business", "font", "times"))

text_df <- text_df %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  anti_join(custom_stop_words, by=c("word"="word")) %>%
  arrange(id)
```

TODO: more stop words

TODO: more cleaning


```{r}
head(text_df)
```

Count the top words.

```{r}
text_df %>% group_by(word) %>%
  summarize(count=n()) %>%
  mutate(freq = count / sum(count)) %>%
  arrange(desc(count))
```


Count the top words by status.

```{r}
top_word = text_df %>% group_by(status, word) %>%
  summarize(count=n()) %>%
  mutate(freq = count / sum(count)) %>%
  arrange(desc(count))

top_word
```

Calculate the log ratio:

```{r}
TODO
top_word %>%
  spread(word, count) 
```

%>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))


```{r}
c = text_df %>%
  group_by(id, word) %>%
  summarize(count = n())

c

  count(word, sort = TRUE)
```

```{r}
text_df %>%
  bind_tf_idf(word, id, n)
```


## Latent Dirichlet Allocation


```{r}

c

a = c %>%
  cast_dtm(id, word, count)

a

ap_lda <- LDA(a, k = 20, control = list(seed = 1234))
ap_lda

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```


```{r,fig.width=10,fig.height=11}
ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol=3) +
  coord_flip()
```


```{r}
beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread

```





# Descriptive Statistics

It’s always a good idea to get familiar with the data by understanding the variables. How many are there, what are their types, are there any special notes? Let’s do just that.
 
The dataset contains the following 12 variables for each passenger. 


## Variable: sector

Below is a cross tabulation for the variables sector (rows) and status (columns), along with sums of each row and column.

```{r}
addmargins(table(df$sector, df$status, dnn=c("sector", "status")))
```

 
Below is a graphical representation of the same tabulation, color-coded by those who paid (blue, bottom) and those who did not (orange, top):

```{r}
qplot(sector, data=df, geom="bar", fill=status, xlab="sector")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

## Variable: country

```{r}
addmargins(table(df$country, df$status, dnn=c("country", "status")))
```

```{r}
qplot(country, data=df, geom="bar", fill=status)+
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```


## Variable: gender

```{r}
addmargins(table(df$gender, df$status, dnn=c("gender", "status")))
```

```{r}
qplot(gender, data=df, geom="bar", fill=status)
```

## Variable: nonpayment

```{r}
addmargins(table(df$nonpayment, df$status, dnn=c("nonpayment", "status")))
```

```{r}
qplot(nonpayment, data=df, geom="bar", fill=status)
```

## Variable: loan_amount


```{r}
df %>% 
  ggplot(aes(loan_amount)) +
  geom_density(fill = "orange")
```


```{r}
df %>% 
  ggplot(aes(loan_amount, colour=status, fill=status)) +
  geom_density(alpha=0.1) 
```


```{r}
df %>% 
  ggplot(aes(loan_amount, colour=status, fill=status)) +
  geom_density(alpha=0.1, position="fill") 
```

```{r}
df$loan_amount.cut = cut(df$loan_amount, breaks=c(0, 300, 600, 900, 1500))
addmargins(table(df$loan_amount.cut, df$status, dnn=c("loan_amount.cut", "status")))
```



## Variable: en

TODO

# Building a Classifier Model

Now that we have explored the data, it’s time to dive deeper. Which variable(s) are the biggest predictors of status? This is where classifier models shine. They can tell us exactly how all the variables relate to each other, and which are most important.
 
A decision tree is a popular classifier model in analytics. Here, the decision tree is automatically created by a machine learning algorithm as it learns simple decision rules from the data. These automatically-learned rules can then be used to both understand the variables and to predict future data. A big advantage of decision trees over other classifier models is that they are relatively simple for humans to understand and interpret.
 
A decision tree consists of nodes. Each node splits the data according to a rule. A rule is based on a variable in the data. For example, a rule might be “Age greater than 30.” In this case, the node splits the data by the age variable; those passengers that satisfy the rule (i.e., are greater than 30) follow the left path out of the node; the rest follow the right path out of the node. In this way, paths from the root node down to leaf nodes are created, describing the fate of certain types of passengers.
 
A decision tree path always starts with a root node (node number 1), which contains the most important splitting rule. Each subsequent node contains the next most important rule. After the decision tree is automatically created by the machine learning algorithm, one can use the decision tree to classify an individual by simply following a path: start at the root node and apply each rule to follow the appropriate path until you hit an end.
 
When creating a decision tree from data, the analyst can specify the number of nodes for the machine learning algorithm to create. More nodes leads to a more accurate model, at the cost of a more complicated and harder-to-interpret model. Likewise, fewer nodes usually leads to a less accurate model, but the model is easier to understand and interpret. 
 
First thing's first, let's split the data into training and testing.

```{r}
set.seed(123)
train <- sample_frac(df, 0.8)
test <- setdiff(df, train)
```

## A model with the text

Let's create the model. Let's shoot for fewer nodes, and therefore a simpler and less accurate model. Later, we'll look at a decision tree with more nodes. 

```{r}
form = as.formula(status ~ sector + country + gender + loan_amount + nonpayment)
tree <- rpart(form, train, method="class")
```


The textual rendering contains the node number, the rule, the number of rows that matched this rule, the deviance of this rule, and finally, the probability of survival at this node.

```{r}
tree
```


```{r}
printcp(tree)
```

Let's look at a graphical rendering of the decision tree.

```{r}
rpart.plot(tree, extra=2)
```

Let's use the classifier to predict the class of the testing data.


```{r}
predicted = predict(tree, test, type="class")
```

Let's look at the confusion matrix.

```{r}
actual = test$status
table(actual, predicted)
```

Let's check the accuracy and other metrics of the classifier on the testing data.

```{r}
print(sprintf("Accuracy:    %.3f", Accuracy(y_true=actual, y_pred=predicted)))
print(sprintf("Precision:   %.3f", Precision(y_true=actual, y_pred=predicted)))
print(sprintf("Recall:      %.3f", Recall(y_true=actual, y_pred=predicted)))
print(sprintf("F1 Score:    %.3f", F1_Score(predicted, actual)))
print(sprintf("Sensitivity: %.3f", Sensitivity(y_true=actual, y_pred=predicted)))
print(sprintf("Specificity: %.3f", Specificity(y_true=predicted, y_pred=actual)))
```


# The Challenge


 
# Questions

After digesting the above decision tree, answer the following questions.
 
- What is the most important variable in the decision tree? That is, which is the most important variable for predicting survival?
 
- Which path in the tree has the highest probability of survival?
 
- Which paths in the tree have the lowest probability of survival?
 
- What would be the chances of survival of a third class female?
 
- Of all third class females, what is the effect of ticket fares?
 
- Of males that are younger than 6.5 years old, which variable most predicted their fate?
 
- Which path is taken by the most passengers?
 
- Using the decision tree model, what would be the most likely fate for the following passenger?
ID=891, Pclass=3, Name=Mr. Patrick Dooley, Sex=male, Age=32, Sibsp=0, Parch=0, TicketNumber=370376, Fare=7.75, Cabin=N/A, Embarked=Q, Title=Mr, Fsize=1, Child=Adult, Mother=NotMother


