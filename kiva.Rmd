---
title: 'Using Text Analytics to Predict Loan Default'
subtitle: "A case study of Kiva"
author: "Dr. Stephen W. Thomas, Smith School of Business, Queen's University"
date: "January 2018"
documentclass: article
output:
  pdf_document:
    highlight: pygments
    number_sections: no
    toc: no
    toc_depth: '2'
  word_document:
    toc: no
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, fig.align='center')
```

```{r}
library(tidyverse)
library(ggthemes)
library(scales)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(MLmetrics)
library(topicmodels)
library(tidytext)
library(knitr)
library(kableExtra)
```


Kiva Microfunds[^1]  is a non-profit that allows people to lend money to low-income entrepreneurs and students around the world. Since starting in 2005, Kiva has crowd-funded millions of loans with a repayment rate of 98% to 99%.

[^1]: https://www.kiva.org/

At Kiva, each loan request includes both traditional demographic information, such as gender and location, as well as personal stories on each borrower because Kiva wants lenders to connect with the borrowers on a human level. An example of a personal story:

*Evelyn is 40 years old and married with 3 kids. She is in the Karura Hope women group and her life has been changed by the first KIVA loan she received last year which she is completing this quarter. Before she received the loan, she used to sell 9 litres of milk daily to local residents. After receiving the loan she bought iron sheets, five cement packets, one lorry of sand, some ballast and animal feed for her cows and improved her cow shed. Today she sells a daily average of 40 litres of milk to the Kiamba Dairy cooperative society, which is affiliated to the Kenya Cooperative Creameries at a cost of USD 0.28 per litre. Her daily farming has really grown. Evelyn intends to buy another dairy cow and a tank of water for home consumption and for her cows. She intends to repay in monthly installments.*

Despite her uplifting story, and her previous successful loan, Evelyn defaulted on her next loan of 900 USD. Could this default have been predicted and avoided? 

Kiva's data science team is especially curious if the personal stories hold any predictive value. Text Analytics provides ways to understand and organize text data. Can Kiva's data science team use Text Analytics to leverage the personal stories to build a prediction system for loan applications, thereby reducing risk for lenders and Kiva?


# Kiva's Data

The key objects in the Kiva world are:

- **Loan**. A loan is the most important concept at Kiva. Most other concepts are in some way related to a loan.

- **Borrower**. A borrower is someone who has requested a loan. Borrowers are often
referred to as *businesses* or *entrepreneurs* in order to emphasize the entrepreneurial spirit of
these individuals who work to make a difference in their lives. 

- **Lender**. A lender is a user registered on the Kiva website for the purposes of lending money and
participating in the community. Some lenders have public profiles, known as lender pages, on
the Kiva website, where they can share details about their activities and mission. Most lenders,
however, refrain from displaying their public information and are referred to as “anonymous.”

- **Partner.** A partner, or Kiva field partner, is usually a microfinance institution with which Kiva works to find and fund loans. Every loan at Kiva is offered by a partner to a borrower, and the partner works with Kiva to get funding for that loan from lenders.


```{r}
tmpdf = data.frame(Variable=c("id", "sector", "country", "gender", "loan_amount", "non_payment", "story", "status"),
                   Description=c("A unique identifier for the loan.",
                                 "Industry sector of borrower.",
                                 "Borrower's country of residence.",
                                 "Borrower's gender.",
                                 "Amount of the loan, in USD.",
                                 "Who is liable if the loan defaults: the lender, or the partner?",
                                 "Borrower's personal story.",
                                 "Whether borrower defaulted or repaid loan."
                                 ))
kable(tmpdf, "latex", booktab=TRUE) %>%  
  kable_styling(font_size = 10, full_width=TRUE, latex_options = c("striped", "scale_down"))
```

Kiva's data science team decides to use a snapshot that contains about 8,000 completed loans, of which about 50% were repaid and 50% were defaulted. Exhibit contains a descriptive summary of each variable.


# Beyond Numbers: Adding Text Variables

The overall goal of the data science team is to reduce the risk to potential lenders, and to make sure loan resources are spent on borrowers with the highest likelihood of repaying. The team decides to build a prediction model (i.e., a _classifier_) that uses the available historical data -- numerical, categorical, and textual data -- in order to build an accurate model of which borrowers will repay, and which will default.

Most classifiers are able to accept numerical and categorical data as inputs, but are not able to accept raw text data. As an example, consider the following data, in which the `loan_purpose` variable contains raw text data and thus cannot be input to classifiers.

```{r}
tmpdf = data.frame(id=c(1, 2, 3, 4, "..."), 
                   age=c(34, 56, 23, 29, ""), 
                   country=c("Panama", "Mexico", "Uganda", "Kenya", ""),
                   loan_purpose=c("I'd like to purchase a new farm tractor for my farm.", 
                                 "Hire additional day labourors to tend to my expanding avocado farm.", 
                                 "My child sitting service is expanding and I really need to renovate my home.",
                                 "To purchase 20 more cows for my dairy farm.", ""))

kable(tmpdf, "latex", booktab=TRUE) %>%  
  kable_styling(font_size = 10, full_width=TRUE, latex_options = c("striped", "scale_down"))
```

There are several text analytics techniques to turn a text document into numerical data. One common technique is to simply split each document into individual words, and count how many times each word appears in each document. Doing so to the example data above would result in the following data:


```{r}
tmpdf = data.frame(id=c(1, 2, 3, 4, "..."), 
                   age=c(34, 56, 23, 29, ""), 
                   country=c("Panama", "Mexico", "Uganda", "Kenya", ""),
                   farm=c(2, 1, 0, 1, ""),
                   purchase=c(1, 0, 0, 1, ""),
                   expanding=c(0, 1, 1, 0, ""),
                   cows=c(0,0,0,1, ""),
                   hire=c(0 ,1, 0, 0, ""),
                   tractor=c(1, 0, 0, 0, ""),
                   "..."=c("...","...","...","...", "")
)

kable(tmpdf, "latex", booktab=TRUE) %>%  
  kable_styling(font_size = 10, full_width=TRUE, latex_options = c("striped", "scale_down")) 
```

Each unique word from the raw text data now becomes a variable, and the value of each variable is the number of times that word appeared in the original document.  In this way, the data is now numerical, and thus it can be input to classifiers.

Another common technique is to use two- or three-word phrases, instead of individual words. The idea is the same: the phrases now become variables in the dataset. 

A recent text analytics advancement is to use a _topic model_ to extract the topics, or themes, from each document. In particular, a common topic model is called Latent Dirichlet Allocation (LDA). LDA can automatically extract high-level topics from text documents. For example, LDA might determine that one document contains the topics "dairy, cow, milk" and "wheat, farm, field", while another document contains the topics "clothing, garmet, shirt" and "children, child, kid." LDA will discover which topics are in which documents. Note that the LDA creates the topics from scratch, based on the input text; it does not have a list of predefined topics from which it chooses. As a result, the topics are specific to the text data in the given dataset.

The data science team uses the LDA topic model to extract topics from each document. Exhibit 2 shows the resulting topics, and Exhibit 3 shows a few sample rows in the final dataset.


# Building the Classifiers

Armed with the numeric, categorical, and text (i.e., topics) variables, the data science team can now build a classifier to predict loan defaults. There are lots of classifier algorithms available, from simple logistic regression, to Naive Bayes, all the way to deep neural networks. For this task, the team decides to use a decision tree algorithm, because of its speed, accuracy, and interpritability. 

The team builds two decision tree classifiers. One is trained only on the numerical and categorical variables. The other is trained on the numerical, categorical, and textual (i.e., topics) variables. This will allow the team to determine whether adding text to the classifiers is beneficial. Exhibit 4 shows the two classifiers and their performance.


# Case Discussion Questions

1. What is Kiva's value proposition?
2. What is the primary innovation that Kiva brought to its industry?
3. How does Kiva make money?
4. What factors might go into a lender's decision to lend money to a borrower?
5. How does text data affect a prediction model's ability to predict a default? 
6. Which words/phrases are most biased towards defaulting? Is this expected/intuitive?
7. According to the decision tree models, which variables best predict a default?
8. What additional information might lead to a better prediction model?
9. How would you operationalize the prediction model at Kiva? What technical challenges and risks do you envision? What procedural challenges and risks do you envision?



<!--
# Loading the Data
-->


```{r, include=FALSE}
df <- read_csv("data/kiva.csv")
df = df %>%
  rename(story = en)
```


```{r, include=FALSE}
str(df)
df$id = 1:nrow(df)
df$status = as.factor(df$status)
df$sector = as.factor(df$sector)
df$country = as.factor(df$country)
df$gender = as.factor(df$gender)
df$nonpayment = as.factor(df$nonpayment)
```

<!--
Let's look at a sample of our data.
-->

```{r, include=FALSE}
head(df, n=20)
summary(df)
```

<!--
# Data Cleaning
-->

```{r, include=FALSE}
# Remove HTML Tags
df = df %>% 
  mutate(story = gsub("<.*?>", "", story))

# Convert into tidytext format
text_df <- df %>%
  select(id, status, story) %>%
  unnest_tokens(word, story)

## Remove stopwords
custom_stop_words = data.frame(word=c("loan", "business"))
text_df <- text_df %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  anti_join(custom_stop_words, by=c("word"="word")) %>%
  arrange(id)

# Stem words
#library(SnowballC)
#df = df %>% 
#  mutate(story = wordStem(story))
```


<!-- 
# Feature Engineering

## Latent Dirichlet Allocation

Let's use a technique called Latent Dirichlet Allocation (LDA) to extract the topics from each document.
-->

```{r, include=FALSE}
# Count each word in each document.
word_counts = text_df %>%
  group_by(id, word) %>%
  summarize(count = n())
```


```{r, include=FALSE}
# Create a document term matrix
dtm = word_counts %>% cast_dtm(id, word, count)

# Remove sparse terms from the document term matrix.
library(tm)
dtm2.nosparse <- removeSparseTerms(dtm, 0.9995)

rowTotals <- apply(dtm2.nosparse, 1, sum) #Find the sum of words in each Document
which(rowTotals==0)
dtm.new   <- dtm2.nosparse[rowTotals> 0, ] 
```

<!--
Run the LDA model.
-->

```{r, include=FALSE}
num_topics = 12

# Because the LDA model can take quite a few minutes to run, and because I run this script over and over again
# checking its knitr output, I don't want to run LDA every single time. 
runModel = FALSE
if (runModel == TRUE) {
  # Run the model
  lda <- LDA(dtm.new, k = num_topics, control = list(seed = 1234))
  
  # Name each topic
  t = terms(lda, k=4)
  topic_names = apply(t, 2, function(x) paste(x, collapse = "_"))
  
  lda_beta <- tidy(lda, matrix = "beta")
  lda_gamma <- tidy(lda, matrix = "gamma")
  lda_gamma$document = as.integer(lda_gamma$document)
  
  # Save output
  readr::write_csv(beta, sprintf("beta_%d.csv", num_topics))
  readr::write_csv(lda_gamma, sprintf("gamma_%d.csv", num_topics))
  readr::write_csv(as.data.frame(topic_names), sprintf("topicnames_%d.csv", num_topics))
  
} else {
  # Read the output from a previous run
  lda_beta = readr::read_csv(sprintf("beta_%d.csv", num_topics))
  lda_gamma = readr::read_csv(sprintf("gamma_%d.csv", num_topics))
  topic_names = t(readr::read_csv(sprintf("topicnames_%d.csv", num_topics)))
}

tn = data.frame(id=1:12, topic_name = as.character(t(topic_names)))
tn$topic_name = as.character(tn$topic_name)
tn$topic_name = sprintf("%02d: %s", 1:12, tn$topic_name)
  
```

<!--
Add the resulting document topic probabilities to the `df` dataframe.
-->

```{r, include=FALSE}
lda_gamma_new = lda_gamma %>% spread(topic, gamma)

df_new  = df %>% left_join(lda_gamma_new, by=c("id" = "document"))
library(data.table)
setnames(df_new, old=sprintf("%d", c(1:12)), new=sprintf("topic %d: %s", c(1:12), topic_names))
```



<P style="page-break-before: always">
\newpage

# Exhibit 1

Below are descriptive plots for the variables in the dataset.

\vspace{20pt}

```{r fig.height=2, fig.width=2, out.width='.49\\linewidth', fig.show='hold',fig.align='center'}

#myt = theme_igray()
myt = theme_economist()
#myt = theme_fivethirtyeight()
qplot(status, data=df, geom="bar", fill=status, xlab="status") + myt +
  theme(legend.position = "none") 

qplot(gender, data=df, geom="bar", fill=status) + myt +
  theme(legend.position = "none")

qplot(nonpayment, data=df, geom="bar", fill=status) + myt +
  theme(legend.position = "none")
```

\vspace{20pt}



```{r fig.height=2.8, fig.width=3, out.width='.49\\linewidth', fig.show='hold',fig.align='center'}

tmpdf = df
tmpdf$country = gsub("Dominican Republic", "D.R.", tmpdf$country)

qplot(country, data=tmpdf, geom="bar", fill=status) + myt +
  theme(legend.position = "none")

qplot(sector, data=df, geom="bar", fill=status, xlab="sector") + myt +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1)) + theme(legend.position = "none")

rm(tmpdf)
```

\vspace{20pt}


```{r fig.width=4, fig.height=2.5, out.width='.49\\linewidth', fig.show='hold',fig.align='center'}
df %>% 
  ggplot(aes(loan_amount, colour=status, fill=status)) + myt +
  geom_density(alpha=0.1) +
  theme(legend.position = "none")

df %>%
  mutate(en_length = nchar(story)) %>%
  ggplot(aes(en_length, colour=status, fill=status)) + myt +
  geom_density(alpha=0.1) +
  theme(legend.position = "none") +
  labs(x = "Number of letters in `story`")
```


<P style="page-break-before: always">
\newpage

### Top Words

The top (i.e, most frequently occuring) words in the `story` variable.

```{r rows.print=20}
kable(text_df %>% group_by(word) %>%
  summarize(count=n()) %>%
  mutate(freq = count / sum(count)) %>%
  arrange(desc(count)) %>%
  top_n(18), "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```


### Most Biased Words

The plots below show which words are most biased towards being `paid` or `defaulted`, using the log odds ratio metric.

```{r}
status_words_count = text_df %>% group_by(status, word) %>%
  summarize(count=n()) %>%
  arrange(desc(count))

log_ratios = status_words_count %>% 
  spread (status, count) %>%
  select(-`<NA>`) %>%
  mutate(defaulted = ifelse(is.na(defaulted), 0, defaulted)) %>%
  mutate(paid = ifelse(is.na(paid), 0, paid)) %>%
  mutate(total=defaulted+paid) %>%
  mutate(log_ratio = log2(paid/defaulted)) 
```

```{r, fig.height=3.5}
log_ratios %>%
  filter(total > 500) %>%
  group_by(log_ratio < 0) %>%
  top_n(14, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio)) %>%
  ggplot(aes(word, log_ratio, fill = log_ratio < 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio") +
  scale_fill_discrete(name = "", labels = c("paid", "default"))
```


```{r, include=FALSE}
kable(log_ratios %>%
  filter(total > 500) %>%
  arrange(desc(log_ratio)) %>%
  top_n(17), "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```


```{r rows.print=20, include=FALSE}
kable(log_ratios %>%
  filter(total > 500) %>%
  arrange((log_ratio)) %>%
  top_n(-20), "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```



<P style="page-break-before: always">
\newpage

# Exhibit 2: LDA Topics

Latent Dirichlet Allocation (LDA) was applied to the `story` variable using the R package `topicmodels`. The Kiva team told LDA to find twelve topics.

### LDA Top Terms Per Topic

This figure shows the top terms (words) in each of the twelve discovered topics. Each topic is assigned a name of the form "Topic Number: TopWord1_TopWord2_TopWord3_TopWord4."

```{r,fig.width=10,fig.height=8.0}
ap_top_terms <- lda_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  left_join(tn, by=c("topic" = "id")) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic_name, scales = "free", ncol=3) +
  coord_flip()
```


<P style="page-break-before: always">
\newpage

### Documents per LDA Topic

The figure below shows the number of documents that contain each topic.

```{r}
topic_totals = lda_gamma %>%
  left_join(df, by=c("document" = "id")) %>%
  select(c(-story)) %>% 
  filter(gamma >= 0.05) %>%
  group_by(topic, status) %>% 
  summarize(count=n()) %>%
  spread(status, count) %>%
  mutate(total = defaulted + paid) %>% 
  left_join(tn, by=c("topic" = "id")) %>%
  select(topic, topic_name, everything())
```

```{r fig.height=3}
tmp_gathered = topic_totals %>% 
  select(topic, topic_name, defaulted, paid) %>% 
  gather(Status, Value, defaulted, paid)

tmp_gathered$topic = as.factor(tmp_gathered$topic)
ggplot(tmp_gathered, aes(x=topic, y=Value, fill=Status)) + myt +
  geom_bar(stat="identity")
```


```{r, include=FALSE}
kable(topic_totals, "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```


### LDA Examples

Below is an example of a `story` variable contained LDA topic 8 at 99%. 

*Senaida Agueda has a business in which she buys and resells clothing that she has been operating for about 2 years now with the help of loans from Esperanza. She has two children, 41 and 38, whom do not live with Mrs. Agueda. When not working with her clothing shop, she enjoys going to the beach and cooking, arroz con carne (rice with meat) being one of her favorite dishes. As Mrs. Agueda is in her elder years and has fully grown children, she simply wishes to sustain her business to support her and her husband. Mrs. Senaida Agueda is a member of an eight person group, Group 4, that is part of a larger micro-bank called Mujeres de Fe, "Women of Faith" in English. In the picture, Mrs. Agueda is third from the right along with members of her group and some others of Mujeres de Fe. (...)*

Below is an example for LDA topic 4: 

 *Descripcin del Negocio. La Sra Angela se dedica a la venta de articulos para el hogar a credito y de forma anbulante ademas vende golosinas a llos nios de una escuela. En la actualidad vende a personas de otros sectores que han sido recomendados por buenos clientes que le refieren esos sitios. Uso del Prstamo. Ella necesita el credito para comprar mas mercaderia pues en estas epocas de fin de aos le son muy solicitadas. Informacin Personal. Ella tiene 29 aos y tiene dos hijos que estudian su casa es de caa y tiene estabilidad familiar. Translated from Spanish by Kiva Volunteer Wendy Wise*


And finally, below is an example for LDA topic 1:

*Mary is 65 years of age, married with six children. All her children are married and self-reliant. She is a member of St Jude group at Githunguri in Thika district. Mary is earns her income as a dairy farmer. She needs a USD 150 loan to help her buy another small high breed dairy calf, which she will raise to maturity. She plans to meet her repayments on monthly basis.*


```{r, eval=FALSE, include=FALSE}
ids = lda_gamma %>%
  filter(topic==1) %>%
  arrange(desc(gamma)) %>%
  top_n(1000) %>%
  left_join(df_new, by=c("document" = "id")) %>%
  mutate(len = nchar(story)) %>%
  arrange(len)
ids
ids = c(7830, 7306, 7258, 7105)

df[7306,3]

df_new[ids,]

kable(t(df_new[ids[1],]), "latex", booktab=TRUE)  %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```


<P style="page-break-before: always">
\newpage

# Exhibit 3: A Sample of Data

A few random rows of the final dataset. The columns named "T1", "T2", etc., are the percentage of the original story's words that have been assigned to topic 1, topic 2, etc., as determined by the LDA topic model. 

\vspace{20pt}

```{r}
ids = c(1, 560, 709, 1111, 1278, 2345, 3984, 4000, 5000, 6000, 7000, 7566)
set.seed(124)
ids = floor(runif(60, min = 1, max = nrow(df_new)))

aa = df_new[ids,] %>%
  select(-story, -id)

# Just to make the table look better
aa$country = gsub("Dominican Republic", "D.R.", aa$country)
setnames(aa, new=sprintf("T%d", c(1:12)), old=sprintf("topic %d: %s", c(1:12), topic_names))

kable(aa, "latex", booktab=TRUE, digits=2) %>%
  kable_styling(latex_options = c("striped", "scale_down"))
```


 
<P style="page-break-before: always">
\newpage

# Exhibit 4: Model Perfomance

Decision tree models were created with the R package `rpart`.

## Model 1 (No text)

Below is the model that was learned from the training data, excluding the text (i.e., topic variables).

```{r fig.height=2}
set.seed(123)
# Don't want to use either of these for prediction, and the - sign doesn't work
# with rpart forumulas.
df_notext = subset(df_new, select=c(status, sector, country, gender, loan_amount, nonpayment))

# Split the data into training and testing.
train_notext <- sample_frac(df_notext, 0.8)
test_notext <- setdiff(df_notext, train_notext)


# Let's train the model.
form = as.formula(status ~ .)
tree <- rpart(form, train_notext, method="class")
rpart.plot(tree, extra=2)
```


<!--
\vspace{20pt}

The following table summarizes the predictions of the decision on testing data.
-->

```{r, eval=TRUE, inculde=FALSE}
predicted = predict(tree, test_notext, type="class")
actual = test_notext$status
preds = data.frame((table(predicted, actual))) %>%
  spread(actual, Freq) %>%
  mutate(total = defaulted + paid) %>%
  select(predicted, total, everything())
```

```{r, include=FALSE}
kable(preds, "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down")) %>%
  add_header_above(c(" " = 2, "actual" = 2))
```


## Model 2 (With Text)

Below is the model that was learned from the training data, including all variables.


```{r fig.height=2.5}
set.seed(123)
# Don't want to use either of these for prediction, and the - sign doesn't work
# with rpart forumulas.
df_text = subset(df_new, select=c(-id, -story))

# Split the data into training and testing.
train_text <- sample_frac(df_text, 0.8)
test_text <- setdiff(df_text, train_text)


# Let's create the model.
form = as.formula(status ~ .)
tree <- rpart(form, train_text, method="class")
rpart.plot(tree, extra=2)
```

<!--
\vspace{20pt}

Below is a summary of its predictions:
-->

```{r, include=FALSE}

predicted.text = predict(tree, test_text, type="class")
actual.text = test_text$status
preds.text = data.frame((table(predicted.text, actual.text))) %>%
  spread(actual.text, Freq) %>%
  mutate(total = defaulted + paid) %>%
  select(predicted.text, total, everything())
```



```{r, include=FALSE}
kable(preds.text, "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down")) %>%
  add_header_above(c(" " = 2, "actual" = 2))
```


<!--
That is, the model predicted `defaulted` 74 times: 50 times correctly, and 24 times incorrectly. It predicted `paid` 159 times: 120 times correctly, and 39 times incorrectly.
-->


## Metrics

Below is the accuracy and other metrics of the two models.

```{r}
bb = data.frame(Metric=c("Accuracy", "Precision", "Recall", "F1 Score", "Sensitivity", "Specificity"),
                "Model_1" =c(Accuracy(y_true=actual, y_pred=predicted),
                        Precision(y_true=actual, y_pred=predicted),
                        Recall(y_true=actual, y_pred=predicted),
                        F1_Score(predicted, actual),
                        Sensitivity(y_true=actual, y_pred=predicted),
                        Specificity(y_true=predicted, y_pred=actual)),
                "Model_2" =  c(Accuracy(y_true=actual.text, y_pred=predicted.text),
                        Precision(y_true=actual.text, y_pred=predicted.text),
                        Recall(y_true=actual.text, y_pred=predicted.text),
                        F1_Score(predicted.text, actual.text),
                        Sensitivity(y_true=actual.text, y_pred=predicted.text),
                        Specificity(y_true=predicted.text, y_pred=actual.text)))


kable(bb, "latex", booktab=TRUE, digits=3) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```




<P style="page-break-before: always">
\newpage


# Appendix 1: Data Collection

The data in this case study was collected from Build.Kiva[^2], Kiva's website that provides snapshots of Kiva loan data. In the full dataset, about 98% of loans are paid and 2% defaulted. In this case study, we look at only a sample of the data, where the split between paid and defaulted is closer to 50%-50%. This sample is available at [http://www.github.com/stepthom/sandbox/data/kiva.csv](http://www.github.com/stepthom/sandbox/data/kiva.csv).

[^2]: https://build.kiva.org

 
