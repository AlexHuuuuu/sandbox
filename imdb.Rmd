---
title: "Text Classification Case Study: IMDB Dataset"
author: "Dr. Stephen W. Thomas, Queen's University"
date: "2017"
output:
  pdf_document:
    highlight: pygments
    number_sections: yes
    toc: no
    toc_depth: '2'
---



```{r}
library(tidytext)
library(RSentiment)
library(cleanNLP)
library(tidyr)
library(dplyr)
library(ggplot2)
library(readr)
library(lubridate)
library(stringr)
library(scales)
library(tm)
```

Read in the data.

```{r}
imdb = read_delim("all.imdb.pipe.csv", delim="|", quote="")

imdb <- imdb %>%
  mutate(id = 1:nrow(imdb)) %>%
  select(id, everything())

imdb <- imdb %>%
  sample_frac(0.03)

head(imdb)

```

```{r}
# Investigating  what uid means.

imdb %>%
  group_by(uid) %>%
  summarize(count = n()) %>% arrange(desc(count))

imdb %>%
  filter(uid == 24)

imdb %>%
  group_by(sentiment) %>%
  summarize(count=n())
```


Convert to tidy text

```{r}
tidy <- imdb %>% 
  unnest_tokens(word, review)

tidy
```


# Some descriptive stats
```{r}
# Word frequences
word_freqs = tidy %>%
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
  
# Show most frequent words  
word_freqs %>%
  top_n(50)
```


```{r}
# Show least frequent words  
word_freqs %>%
  top_n(-50)
```

```{r}
# Sentiment distribution
tidy %>%
  group_by(sentiment) %>%
  summarize(n = n()) %>%
  mutate(freq= n/sum(n))
```

```{r}
# Most positive and negative words
sentiment_words_count = tidy %>% 
  group_by(sentiment, word) %>%
  summarize(count=n()) %>%
  arrange(desc(count))
```

```{r}
log_ratios = sentiment_words_count %>% 
  spread (sentiment, count) %>%
  mutate(negative = ifelse(is.na(negative), 0, negative)) %>%
  mutate(positive = ifelse(is.na(positive), 0, positive)) %>%
  mutate(total=negative+positive) %>%
  mutate(log_ratio = log2(positive/negative)) 

log_ratios %>%
  filter(total > 50) %>%
  group_by(log_ratio < 0) %>%
  top_n(15, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio)) %>%
  ggplot(aes(word, log_ratio, fill = log_ratio < 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio") +
  scale_fill_discrete(name = "", labels = c("positive", "negative"))
```

```{r}
# Log odd ratio chart for n-grams

tidy_tri <- imdb %>% 
  unnest_tokens(word, review, token="ngrams", n=2)

# Most positive and negative words
sentiment_words_count_tri = tidy_tri %>% 
  group_by(sentiment, word) %>%
  summarize(count=n()) %>%
  arrange(desc(count))

log_ratios_tri = sentiment_words_count_tri %>% 
  spread (sentiment, count) %>%
  mutate(negative = ifelse(is.na(negative), 0, negative)) %>%
  mutate(positive = ifelse(is.na(positive), 0, positive)) %>%
  mutate(total=negative+positive) %>%
  mutate(log_ratio = log2(positive/negative)) 


log_ratios_tri %>%
  filter(total > 50) %>%
  arrange(desc(log_ratio))


log_ratios_tri %>%
  filter(total > 100) %>%
  group_by(log_ratio < 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio)) %>%
  ggplot(aes(word, log_ratio, fill = log_ratio < 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio") +
  scale_fill_discrete(name = "", labels = c("positive", "negative"))
```


# Convert to tm format

Create DTM
```{r}
tidy_counts = tidy %>%
  group_by(id, uid, sentiment, word) %>%
  summarize(count = n())

tidy_counts

dtm <- tidy_counts %>%
  cast_dtm(id, word, count)

tidy_counts %>%
  filter(id==57)

inspect(dtm[1:10, 1:20])


dtm <- removeSparseTerms(dtm, 0.7)
dtm

df <- as.data.frame(as.matrix(dtm))
df

# Get the truth label
labels = tidy %>%
  group_by(id, sentiment) %>%
  select(id, sentiment)

labels
df_l = merge(df, labels, by="row.names", all.x=TRUE)


```


Build classifier
```{r}

```