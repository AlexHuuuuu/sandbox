---
title: "Text Classification Case Study: IMDB Dataset"
author: "Dr. Stephen W. Thomas, Queen's University"
date: "2017"
output:
  pdf_document:
    highlight: pygments
    number_sections: yes
    toc: no
    toc_depth: '2'
---



```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidytext, tidyverse, cleanNLP, lubridate, 
               stringr, scales, tm, rpart, rpart.plot, MLmetrics, stringi)
```

# Read in the data

```{r}
imdb = read_delim("all.imdb.pipe.csv", delim="|", quote="")

# Give each row a unique id
imdb <- imdb %>%
  mutate(id = 1:nrow(imdb)) %>%
  select(id, everything())

dim(imdb)

# For now, just take a fraction
set.seed(123)
imdb <- imdb %>%
  sample_frac(0.3)

# Now, I noticed that there were some weird characters in the file. Non-ascii, weird stuff.
# I removed some in the input file, in vim, using the command:
# :.,$s/[\x97]/ - /g
#
# I did the above for (if memory serves): \u0096, \u0097, \u0084, \u008d, \u0095, and \u0091.
#
# For good measure, let's remove a few more, here in R, below:

imdb$review = stringi::stri_trans_general(imdb$review, "latin-ascii")
imdb$review= gsub('\\R', '', imdb$review, perl=T)
imdb$review= gsub('£|¢|§|¦|·|¡|°|¿', ' ', imdb$review, perl=T)
imdb$review= gsub('´', '\'', imdb$review, perl=T)
imdb$review= gsub('¨', '"', imdb$review, perl=T)


# BTW, here is how I searched for those non-ascii characters:
# grep("I_WAS_NOT_ASCII", iconv(imdb$review, "latin1", "ASCII", sub="I_WAS_NOT_ASCII"))

# Inspect
head(imdb)
imdb[4,]$review
```

```{r}
# This is not really important. I was just 
# investigating  what uid means.

imdb %>%
  group_by(uid) %>%
  summarize(count = n()) %>% arrange(desc(count))

imdb %>%
  filter(uid == 24)

imdb %>%
  group_by(sentiment) %>%
  summarize(count=n())
```


# Convert to tidy text

```{r}
tidy <- imdb %>% 
  unnest_tokens(word, review)
tidy
```


# Some descriptive stats

Most frequent words.
```{r}
word_freqs = tidy %>%
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
  
word_freqs %>%
  top_n(50)
```


Least frequent words 
```{r}
word_freqs %>%
  top_n(-50)
```


Sentiment distribution
```{r}
tidy %>%
  group_by(sentiment) %>%
  summarize(n = n()) %>%
  mutate(freq= n/sum(n))
```


Most positive and negative words
```{r}
sentiment_words_count = tidy %>% 
  group_by(sentiment, word) %>%
  summarize(count=n()) %>%
  arrange(desc(count))

log_ratios = sentiment_words_count %>% 
  spread (sentiment, count) %>%
  mutate(negative = ifelse(is.na(negative), 0, negative)) %>%
  mutate(positive = ifelse(is.na(positive), 0, positive)) %>%
  mutate(total=negative+positive) %>%
  mutate(log_ratio = log2(positive/negative)) 

# Save this for later, to be used as features in classification:
# the most polarizing words
top_log_ratios = log_ratios %>%
  filter(total > 50) %>%
  group_by(log_ratio < 0) %>%
  top_n(40, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio))

log_ratios %>%
  filter(total > 50) %>%
  group_by(log_ratio < 0) %>%
  top_n(15, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio)) %>%
  ggplot(aes(word, log_ratio, fill = log_ratio < 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio") +
  scale_fill_discrete(name = "", labels = c("positive", "negative"))
```


Log odd ratio chart for n-grams

```{r}
tidy_tri <- imdb %>% 
  unnest_tokens(word, review, token="ngrams", n=2)

# Most positive and negative words
sentiment_words_count_tri = tidy_tri %>% 
  group_by(sentiment, word) %>%
  summarize(count=n()) %>%
  arrange(desc(count))

log_ratios_tri = sentiment_words_count_tri %>% 
  spread (sentiment, count) %>%
  mutate(negative = ifelse(is.na(negative), 0, negative)) %>%
  mutate(positive = ifelse(is.na(positive), 0, positive)) %>%
  mutate(total=negative+positive) %>%
  mutate(log_ratio = log2(positive/negative)) 

log_ratios_tri %>%
  filter(total > 100) %>%
  group_by(log_ratio < 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio)) %>%
  ggplot(aes(word, log_ratio, fill = log_ratio < 0)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio") +
  scale_fill_discrete(name = "", labels = c("positive", "negative"))
```


# Build a classifier.

Create the feature matrix.

There's a lot of features we can have for each document:
- Word frequencies of all words
- Word frequencies of only the most frequent words
- Word frequencies of only the most polarizing words
- Any of the above, except with binary word frequencies or some other weighting scheme
- Topic memberships (after running e.g., LDA)
- Cluster memberships (after running e.g., kmeans)

For now, we'll go with the third approach. We'll start by creating a DTM.

```{r}


use_tidy = FALSE
if (use_tidy == TRUE) {
  # First, need to get the counts of each polarizing word in each doc
  tidy_counts = tidy %>%
    filter(word %in% top_log_ratios$word) %>% # only keep most polarizing words
    group_by(id, uid, sentiment, word) %>%
    summarize(count = n())
  
  # Make the DTM
  dtm <- tidy_counts %>%
    cast_dtm(id, word, count)
  dim(dtm)
  inspect(dtm)
  
  # Now, if we want to use some of tm's preprocessing functions, we need to first convert the DTM
  # to a corpus object, then run the preprocessing functions, and then convert the corpus object
  # back to a DTM. (Strange, I know.)
  
  # Convert DTM to a list of text
  dtm_list <- apply(dtm, 1, function(x) {
      paste(rep(names(x), x), collapse=" ")
  })
  
  corpus <- VCorpus(VectorSource(dtm_list))
} else {
  sourceData <- VectorSource(imdb$review)
  corpus <- VCorpus(sourceData)
}

corpus[[1]]$content
corpus[[2]]$content
corpus[[3]]$content
corpus[[4]]$content

# Y'all got any more of those preprocessing steps?
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, c("\u0085", "br"))
corpus <- tm_map(corpus, stemDocument, language = "english") 
corpus <- tm_map(corpus, stripWhitespace)

corpus[[1]]$content
corpus[[2]]$content
corpus[[3]]$content
corpus[[4]]$content

BigramTokenizer  <- function(x) {
  RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 3))
}

dtm <- DocumentTermMatrix(corpus, 
                          control = list(tokenize = BigramTokenizer, weighting = function(x) weightTfIdf(x, normalize = FALSE)))


dtm$dimnames$Terms
inspect(dtm)

# Only keep the top features
dtm1 = removeSparseTerms(dtm, .994)
dim(dtm1)
dtm1$dimnames$Terms
```

```{r}

# Convert the dtm to a dataframe, so we can pass it into some classifiers
df <- as.data.frame(as.matrix(dtm))

# Get the truth label.
# How I do this is a bit wonky. What we have to do is append a column to our df 
# above that is the truth label. To do this, we'll:
# - Create a new dataframe with just the id and the label
# - Join that new dataframe to the one above, by id

labels = tidy %>%
  group_by(id) %>%
  summarize(sentiment=max(sentiment)) %>%
  as.data.frame()

rownames(labels) <- labels[,1]
labels[,1] <- NULL
df_l = merge(df, labels, by="row.names", all.x=TRUE)
df_l[,1] <- NULL # Don't want the pesky "RowNames" column hanging around
head(df_l)
```

Split into training and testing.

```{r}
# Training and testing
smp_size <- floor(0.75 * nrow(df_l))
set.seed(123)
train_ind <- sample(seq_len(nrow(df_l)), size = smp_size)

train <- df_l[train_ind, ]
test <- df_l[-train_ind, ]
```

Decision Trees.

```{r}
library(rpart)
library(rpart.plot)
tree <- rpart(sentiment ~ ., data=train)
tree
rpart.plot(tree, extra=2)

predicted = predict(tree, test, type="class")
actual = test$sentiment
(table(predicted, actual))

library(MLmetrics)
print(sprintf("Accuracy:    %.3f", Accuracy(y_true=actual, y_pred=predicted)))
print(sprintf("Precision:   %.3f", Precision(y_true=actual, y_pred=predicted)))
print(sprintf("Recall:      %.3f", Recall(y_true=actual, y_pred=predicted)))
print(sprintf("F1 Score:    %.3f", F1_Score(predicted, actual)))
print(sprintf("Sensitivity: %.3f", Sensitivity(y_true=actual, y_pred=predicted)))
print(sprintf("Specificity: %.3f", Specificity(y_true=predicted, y_pred=actual)))
```


Naive Bayes
```{r}

# TODO: need to first change features to be binary and categorical

library(e1071)
nb <- naiveBayes(sentiment ~ ., data=train)
nb

predicted.nb = predict(nb, test, type="class")
predicted.nb
actual = test$sentiment
(table(predicted.nb, actual))

library(MLmetrics)
print(sprintf("Accuracy:    %.3f", Accuracy(y_true=actual, y_pred=predicted.nb)))
print(sprintf("Precision:   %.3f", Precision(y_true=actual, y_pred=predicted.nb)))
print(sprintf("Recall:      %.3f", Recall(y_true=actual, y_pred=predicted.nb)))
print(sprintf("F1 Score:    %.3f", F1_Score(predicted.nb, actual)))
print(sprintf("Sensitivity: %.3f", Sensitivity(y_true=actual, y_pred=predicted.nb)))
print(sprintf("Specificity: %.3f", Specificity(y_true=predicted.nb, y_pred=actual)))
```