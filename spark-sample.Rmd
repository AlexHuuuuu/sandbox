---
title: "Intro to sparklyr"
---

*Note: this tutorial originally comes from the wonderful people at sparklyr.

Install these packages if you don't have them already

```{r, eval=FALSE}
install.packages(c("nycflights13", "Lahman"))
```

# Connect to Spark

```{r}
library(sparklyr)
sc <- spark_connect(master = "yarn-client", spark_home = "/usr/hdp/current/spark-client/")
```

# Load some data into Spark


```{r}
library(dplyr)
iris_tbl <- copy_to(sc, iris)
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
batting_tbl <- copy_to(sc, Lahman::Batting, "batting")
```

# Manipulate Data

Now, we can use the dplyr verbs (like `filter`) to manipulate Spark DataFrames! 

```{r}
iris_tbl %>% filter(petal_width < 0.3)
```

When this command is run, a lot of magic happens behind the scenes. Spark's implementation of `filter` gets translated into a bunch of MapReduce jobs, and the data is ported around to each job, the jobs are run, and the results are aggregated back.

Let's look at another table:

```{r}
# filter by departure delay
flights_tbl %>% filter(dep_delay == 2)
```

```{r}
# an example from the dplyr library
delay <- flights_tbl %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) %>%
  collect()

delay
```

# Plot some data


```{r}
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```

# Using SQL
```{r}
library(DBI)
iris_preview <- dbGetQuery(sc, "SELECT * FROM iris LIMIT 10")
iris_preview
```

# Building a Decision Tree

# TODO: split into training and testing


```{r}
tree = ml_decision_tree(iris_tbl, Species ~ .)
str(tree)
summary(tree)
predicted = predict(tree, iris_tbl)
predicted

# In Spark, a command like `iris$Species` won't work, unfortunately. 
actual = iris %>% select(Species) %>% collect %>% .[["Species"]]


table(predicted, actual)

# TODO: Other stats
```

# NOTE - due to security configuration, the spark web ui will not work!

```{r}
# now disconnect from spark
spark_disconnect(sc)
```