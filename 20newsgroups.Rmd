---
title: "Text Clustering Case Study: 20 Newsgroups"
author: "Dr. Stephen W. Thomas, Queen's University"
date: "2017"
output:
  pdf_document:
    highlight: pygments
    number_sections: yes
    toc: no
    toc_depth: '2'
---



```{r}
library(tidytext)
library(RSentiment)
library(cleanNLP)
library(tidyr)
library(dplyr)
library(ggplot2)
library(readr)
library(lubridate)
library(stringr)
library(scales)
library(tm)
```


# Create the CSV file

```{r}
# Note: I already ran this; you don't need to run it again.

# Downloaded the tarball from: http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz
dir = '20news-18828'
cats = list.files(file.path(dir))
df = data.frame(matrix(ncol = 3, nrow = 0))
colnames(df) = c("category", "id", "text")
for (cat in cats) {
  print(cat)
  files = list.files(file.path(dir, cat))
  for (file in files){
    fileName = file.path(dir, cat, file)
    print(fileName)
    text = readChar(fileName, file.info(fileName)$size)
    df[nrow(df) + 1,] = c(cat, file, text)
  }
}
head(df)

# Put actual newlines in, not the characters
df$text = gsub("\\n", "\n", df$text)

Encoding(df$text) <- "UTF-8"
df$text = iconv(df$text, "UTF-8", "UTF-8", sub='') ## replace any non UTF-8 by ''

write_csv(df, "20newsgroup.csv")
```


# Read in the data

```{r}
news = read_csv("20newsgroup.csv")
# For now, just take a fraction

set.seed(123)
news <- news %>%
  sample_frac(0.25)

# Inspect
head(news)

```

Filter by category

```{r}
# For now, we'll just use these categories
categories = c('rec.sport.baseball', 'talk.religion.misc', 'comp.graphics', 'sci.space')

news <- news %>%
  filter(category %in% categories)

news %>%
  group_by(category) %>%
  summarize(n = n()) %>%
  mutate(freq= n/sum(n))
```


# Cleaning and Preprocessing

Remove some noise like email headers and footers. This has to be done before tidying the text, because it
is based on regular expression matching, which needs to work with full strings.

```{r}
head(news)

news$text_pre = news$text

news$text_pre <- gsub('^From:.*\n', "", news$text_pre, ignore.case=TRUE, perl=TRUE)
news$text_pre <- gsub('\nin-reply-to:.*\n', "", news$text_pre, ignore.case=TRUE, perl=TRUE)
news$text_pre <- gsub('\n.*writes:\n', "", news$text_pre, ignore.case=TRUE, perl=TRUE) 
news$text_pre <- gsub('\\S+@\\S+|\\{(?:\\w+, *)+\\w+\\}@[\\w.-]+', "", news$text_pre, ignore.case=TRUE, perl=TRUE) 
head(news)


news$text[3]

```



```{r}
corpus <- (VectorSource(news$text_pre))
corpus <- Corpus(corpus)
summary(corpus)

inspect(corpus[1])
inspect(corpus[10])


```

```{r}
custom_stop_words = c('would', 'subject', 'from', 're', 'don', 'jan',
     'feb', 'mar', 'apr', 'may', 'june',
     'july', 'aug', 'sep', 'oct', 'nov', 'dec', 'yes', 'no',  'cant', 'can', 'say', 'said', 'one')

corpus <- tm_map(corpus, content_transformer(tolower))
inspect(corpus[10])
corpus <- tm_map(corpus, content_transformer(removePunctuation))
inspect(corpus[10])
corpus <- tm_map(corpus, stripWhitespace) 
inspect(corpus[10])
corpus <- tm_map(corpus, content_transformer(removeNumbers))
inspect(corpus[10])
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))
inspect(corpus[10])
corpus <- tm_map(corpus, content_transformer(removeWords), custom_stop_words)
inspect(corpus[10])
corpus <- tm_map(corpus, stemDocument)
inspect(corpus[10])


dtm <- DocumentTermMatrix(corpus,
    control = list(weighting =
    function(x)
        weightTfIdf(x, normalize = FALSE),
        stopwords = FALSE))


inspect(dtm)
dtm <- removeSparseTerms(dtm, 0.95)
inspect(dtm)
sort(colnames(df))


df <- as.data.frame(as.matrix(dtm))
set.seed(123)
fit = kmeans(df, centers=5)
cluster_names = lapply(1:nrow(fit$centers), function(x) names(head(sort(fit$centers[x,], decreasing=TRUE), 6)))
cluster_names

sort(colnames(df))
stopwords("english")


dataframe <- data.frame(text=sapply(corpus, identity), 
    stringsAsFactors=F)

dataframe
```


Convert to tidy text.

```{r}
news
tidy <- news %>% 
  select(-text) %>%
  unnest_tokens(word, text_pre)
tidy
```


# Some descriptive stats

Most frequent words.
```{r}
word_freqs = tidy %>%
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
  
word_freqs %>%
  top_n(50)
```


Least frequent words 
```{r}
word_freqs %>%
  top_n(-50)
```




# Build the DTM Matrix
```{r}

tidy

tidy_counts = tidy %>%
  group_by(id, target, word) %>%
  summarize(count = n())

tidy_counts

# Make the DTM
dtm <- tidy_counts %>%
  cast_dtm(id, word, count)


## Remove Stopwords

# Can optionally do some preprocessing using some functions in the tm package
dtm <- removeSparseTerms(dtm, 0.9)
#dtm <- removeNumbers(dtm)
#dtm <- removePunctuation(dtm)

dtm

inspect(dtm)

# Convert the dtm to a dataframe, so we can pass it into some classifiers
df <- as.data.frame(as.matrix(dtm))

dim(df)
head(df)
```


# Run the clustering algorithm

```{r}
dtm <- removeSparseTerms(dtm, 0.9)
df <- as.data.frame(as.matrix(dtm))
fit = kmeans(df, centers=4)
cluster_names = lapply(1:nrow(fit$centers), function(x) names(head(sort(fit$centers[x,], decreasing=TRUE), 6)))
cluster_names
```


# Output some results

```{r}

fit <- skmeans(dtm, 4)
fit

mfrq_words_per_cluster(fit, dtm)


library(cluster) 
table(fit$cluster)
clusplot(df, fit$cluster, color=TRUE, shade=TRUE,  labels=2, lines=0)
```

```{r}
mfrq_words_per_cluster <- function(fit, dtm, first = 10, unique = TRUE){
  
  first=10
  unique=TRUE

  dtm <- as.simple_triplet_matrix(dtm)
  indM <- table(names(fit$cluster), fit$cluster) == 1 # generate bool matrix
  indM
  
  table(names(fit$cluster), fit$cluster)

  hfun <- function(ind, dtm){ # help function, summing up words
    if(is.null(dtm[ind, ]))  dtm[ind, ] else  col_sums(dtm[ind, ])
  }
  frqM <- apply(indM, 2, hfun, dtm = dtm)
  
  frqM
  
  rowSums(frqM > 0) ==1
  
  i = 1
  mat = frqM
  mat
  mat[,i]
  head(sort(mat[, i], decreasing = TRUE), 10)[1]

  #if(unique){
    # eliminate word which occur in several clusters
   # frqM <- frqM[rowSums(frqM > 0) == 1, ] 
  #}
  # export to list, order and take first x elements 
  res <- lapply(1:ncol(frqM), function(i, mat, first)
                head(sort(mat[, i], decreasing = TRUE), first),
                mat = frqM, first = first)
  res
}
```


```{r}
data("crude")

dtm <- DocumentTermMatrix(crude, control =
                          list(removePunctuation = TRUE,
                               removeNumbers = TRUE,
                               stopwords = TRUE))

rownames(dtm) <- paste0("Doc_", 1:20)

library(skmeans)
library(slam)
clus <- skmeans(dtm, 3)
clus

mfrq_words_per_cluster(clus, dtm)
mfrq_words_per_cluster(clus, dtm, unique = FALSE)

```
